{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Behavior-trait analysis, demographic data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting default fontsizes for plots\n",
    "\n",
    "s=20# CHANGE FONTSIZE HERE\n",
    "\n",
    "plt.rc('font', size=s) #controls default text size\n",
    "plt.rc('axes', titlesize=s) #fontsize of the title\n",
    "plt.rc('axes', labelsize=s) #fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=s) #fontsize of the x tick labels\n",
    "plt.rc('ytick', labelsize=s) #fontsize of the y tick labels\n",
    "plt.rc('legend', fontsize=s) #fontsize of the legend\n",
    "plt.rcParams['savefig.facecolor']='white'\n",
    "#import matplotlib as mpl\n",
    "#mpl.rcParams['font.weight']= 'normal'\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read in restricted behavioral data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res_behav_data = pd.read_csv('../data/RESTRICTED_esfinn_11_21_2021_19_19_35.csv')\n",
    "res_behav_data.set_index(\"Subject\", inplace=True)\n",
    "res_behav_data.index = res_behav_data.index.map(str)\n",
    "print(res_behav_data.shape)\n",
    "res_behav_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read in unrestricted behavioral data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "unres_behav_data = pd.read_csv('../data/unrestricted_esfinn_11_21_2021_19_19_13.csv')\n",
    "unres_behav_data.set_index(\"Subject\", inplace=True)\n",
    "unres_behav_data.index = unres_behav_data.index.map(str)\n",
    "print(unres_behav_data.shape)\n",
    "unres_behav_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(res_behav_data.columns)\n",
    "print(unres_behav_data.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Figuring out how ADR_Intn_T was computed:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Intn_cols = ['ASR_Anxd_Raw','ASR_Witd_Raw','ASR_Soma_Raw'] # from (1)https://wiki.humanconnectome.org/display/PublicData/HCP-YA+Data+Dictionary-+Updated+for+the+1200+Subject+Release \n",
    "# and (2)https://aseba.org/wp-content/uploads/2019/02/asrprofile.pdf\n",
    "Intn_sum = res_behav_data[['ASR_Anxd_Raw','ASR_Witd_Raw','ASR_Soma_Raw']].sum(axis = 1).values \n",
    "uneq_rows = res_behav_data.index[Intn_sum != res_behav_data.loc[:,'ASR_Intn_Raw']]#.values)\n",
    "uneq_rows # rows where sumn of Intn_columns don't match the Intn_Raw score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res_behav_data.loc[uneq_rows,Intn_cols] # these non-matching rows are all empty"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "-  meaning that ASR_Intn_Raw is the sum of the raw entries in the 3 columns in Intn_cols"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "asr_cols = [i for i in res_behav_data.columns if i[:3] == 'ASR' ]\n",
    "#colnum = [i for i in range(len(res_behav_data.columns)) if res_behav_data.columns[i][:3] == 'ASR' ]\n",
    "asr_cols"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Intn_cols = ['ASR_Anxd_Pct','ASR_Witd_T','ASR_Soma_T'] # from (1)https://wiki.humanconnectome.org/display/PublicData/HCP-YA+Data+Dictionary-+Updated+for+the+1200+Subject+Release \n",
    "# and (2)https://aseba.org/wp-content/uploads/2019/02/asrprofile.pdf\n",
    "Intn_sum = res_behav_data[['ASR_Anxd_Pct','ASR_Witd_T','ASR_Soma_T']].sum(axis = 1).values \n",
    "uneq_rows = res_behav_data.index[Intn_sum != res_behav_data.loc[:,'ASR_Intn_T']]#.values)\n",
    "uneq_rows # rows where sumn of Intn_columns don't match the Intn_Raw score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res_behav_data[Intn_cols].mean(axis=1)[:3], res_behav_data['ASR_Intn_T'][:3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res_behav_data[['ASR_Anxd_Raw','ASR_Witd_Raw','ASR_Soma_Raw','ASR_Anxd_Pct','ASR_Witd_T','ASR_Soma_T']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Intn_sum[:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res_behav_data.loc[:,'ASR_Intn_T'].values[:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res_behav_data[['Age_in_Yrs', 'ASR_Intn_Raw', 'ASR_Intn_T']]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combine restricted and unrestricted behavioral data into a single dataframe:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "behav_data = pd.concat([res_behav_data, unres_behav_data], axis=1)\n",
    "print(behav_data.shape)\n",
    "behav_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "behav_data['ASR_Intn_T'].describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Behavior-trait analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read in task response data, join into single dataframe, then join this with the larger dataframe:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "behavioral_data_drive = r'/Users/f0053cz/Dropbox (Dartmouth College)/postdoc_Dartmouth/HCP/BehaviorAnalyses/Documented scripts/data/data_for_paper'\n",
    "behavioral_data_drive"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "task_data = pd.read_csv(os.path.join(behavioral_data_drive, '1b_S_NS_responses_per_subj.csv'))\n",
    "task_data.set_index(\"subj_idx\", inplace=True)\n",
    "task_data.index.rename(\"Subject\", inplace=True)\n",
    "task_data.index = task_data.index.map(str)\n",
    "print(task_data.shape)\n",
    "task_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "task_data.iloc[0,:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "task_data2 = pd.read_csv(os.path.join(behavioral_data_drive, '1e_S_NS_pc_uncertainResp_per_subj.csv'))\n",
    "task_data2.set_index(\"subj_idx\", inplace=True)\n",
    "task_data2.index.rename(\"Subject\", inplace=True)\n",
    "task_data2.index = task_data2.index.map(str)\n",
    "task_data2[\"pc_unc_total\"] = (task_data2[\"pc_unc_Mental\"] + task_data2[\"pc_unc_Rand\"])/2\n",
    "print(task_data2.shape)\n",
    "task_data2.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "task_data = task_data.join(task_data2, how='inner')\n",
    "print(task_data.shape)\n",
    "task_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = task_data.join(behav_data, how='inner')\n",
    "print(data.shape)\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Demographics for the paper\n",
    "data.groupby(['Gender']).count()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data['Age_in_Yrs'].describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explore correlations between proportion of \"social\" responses and various trait phenotypes:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def explore_correlation(x, y, data=data):\n",
    "    \"\"\"\n",
    "    Calculates and plots correlation between x and y variables in dataframe `data`, plus distribution of x and y \n",
    "    \"\"\"\n",
    "    sns.set_style(\"white\")\n",
    "    \n",
    "    inds = ~np.isnan(data[x]) & ~np.isnan(data[y]) # find rows where neither x or y is NaN\n",
    "\n",
    "    g = sns.jointplot(x=x, y=y, data=data, kind='reg', color='gray')\n",
    "\n",
    "    # Calculate and print correlations\n",
    "    rp, pp = stats.pearsonr(data[x][inds], data[y][inds])\n",
    "    rs, ps = stats.spearmanr(data[x][inds], data[y][inds])\n",
    "    #g.ax_joint.annotate(f'r_s = {rs:.2f}\\n(p={ps:.1e})', xy=(.05,.8), xycoords='axes fraction')\n",
    "    print(f'Spearman r={rs}, p = {ps}')\n",
    "    \n",
    "    return g"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The following function was taken from here: https://github.com/psinger/CorrelationStats/blob/master/corrstats.py\n",
    "\n",
    "from scipy.stats import t, norm\n",
    "from math import atanh, pow\n",
    "from numpy import tanh\n",
    "\n",
    "def rz_ci(r, n, conf_level = 0.95):\n",
    "    zr_se = pow(1/(n - 3), .5)\n",
    "    moe = norm.ppf(1 - (1 - conf_level)/float(2)) * zr_se\n",
    "    zu = atanh(r) + moe\n",
    "    zl = atanh(r) - moe\n",
    "    return tanh((zl, zu))\n",
    "\n",
    "def rho_rxy_rxz(rxy, rxz, ryz):\n",
    "    num = (ryz-1/2.*rxy*rxz)*(1-pow(rxy,2)-pow(rxz,2)-pow(ryz,2))+pow(ryz,3)\n",
    "    den = (1 - pow(rxy,2)) * (1 - pow(rxz,2))\n",
    "    return num/float(den)\n",
    "\n",
    "def dependent_corr(xy, xz, yz, n, twotailed=True, conf_level=0.95, method='steiger'):\n",
    "    \"\"\"\n",
    "    Calculates the statistic significance between two dependent correlation coefficients\n",
    "    @param xy: correlation coefficient between x and y\n",
    "    @param xz: correlation coefficient between x and z\n",
    "    @param yz: correlation coefficient between y and z\n",
    "    @param n: number of elements in x, y and z\n",
    "    @param twotailed: whether to calculate a one or two tailed test, only works for 'steiger' method\n",
    "    @param conf_level: confidence level, only works for 'zou' method\n",
    "    @param method: defines the method uses, 'steiger' or 'zou'\n",
    "    @return: t and p-val\n",
    "    \"\"\"\n",
    "    if method == 'steiger':\n",
    "        d = xy - xz\n",
    "        determin = 1 - xy * xy - xz * xz - yz * yz + 2 * xy * xz * yz\n",
    "        av = (xy + xz)/2\n",
    "        cube = (1 - yz) * (1 - yz) * (1 - yz)\n",
    "\n",
    "        t2 = d * np.sqrt((n - 1) * (1 + yz)/(((2 * (n - 1)/(n - 3)) * determin + av * av * cube)))\n",
    "        p = 1 - t.cdf(abs(t2), n - 3)\n",
    "\n",
    "        if twotailed:\n",
    "            p *= 2\n",
    "\n",
    "        return t2, p\n",
    "    elif method == 'zou':\n",
    "        L1 = rz_ci(xy, n, conf_level=conf_level)[0]\n",
    "        U1 = rz_ci(xy, n, conf_level=conf_level)[1]\n",
    "        L2 = rz_ci(xz, n, conf_level=conf_level)[0]\n",
    "        U2 = rz_ci(xz, n, conf_level=conf_level)[1]\n",
    "        rho_r12_r13 = rho_rxy_rxz(xy, xz, yz)\n",
    "        lower = xy - xz - pow((pow((xy - L1), 2) + pow((U2 - xz), 2) - 2 * rho_r12_r13 * (xy - L1) * (U2 - xz)), 0.5)\n",
    "        upper = xy - xz + pow((pow((U1 - xy), 2) + pow((xz - L2), 2) - 2 * rho_r12_r13 * (U1 - xy) * (xz - L2)), 0.5)\n",
    "        return lower, upper\n",
    "    else:\n",
    "        raise Exception('Wrong method!')\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data.shape, data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DV1: response % social - % nonsocial"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = \"Soc-NonSoc_pc\"\n",
    "y = \"ASR_Intn_T\"\n",
    "\n",
    "g = explore_correlation(x, y)\n",
    "g.ax_joint.set_xlabel('%\"Social\" - %\"Non-social\"')\n",
    "g.ax_joint.set_ylabel('Internalizing score (T)')\n",
    "#plt.text(0.1,10,r\"$r_{S}=-.58$\"f'\\n(p='r\"$.08^+)$\")\n",
    "g.ax_joint.set_xlim(-100,102)\n",
    "g.ax_joint.annotate(r\"$ r_{S}=.1$\"f'\\n(p='r\"$.003)$\", xy=(.02,.85), xycoords='axes fraction',fontsize=20) # 0.1, .003\n",
    "g.savefig('../results/soc-nonsoc_vs_ASRIntnT.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# externalization score\n",
    "x = \"Soc-NonSoc_pc\"\n",
    "y = \"ASR_Extn_T\"\n",
    "g = explore_correlation(x, y)\n",
    "g.ax_joint.set_ylabel('Externalizing score (T)')\n",
    "g.ax_joint.set_xlabel('%\"Social\" - %\"Non-social\"')\n",
    "g.ax_joint.set_xlim(-100,102)\n",
    "#plt.text(0.1,10,r\"$r_{S}=-.58$\"f'\\n(p='r\"$.08^+)$\")\n",
    "g.ax_joint.annotate(r\"$ r_{S}=.06$\"f'\\n(p='r\"$.096)$\", xy=(.02,.85), xycoords='axes fraction',fontsize=20) # 0.1, .003\n",
    "g.savefig('../results/soc-nonsoc_vs_ASRExntT.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = \"Soc-NonSoc_pc\"\n",
    "y = \"ASR_Intn_T\"\n",
    "z = \"ASR_Extn_T\"\n",
    "\n",
    "inds = ~np.isnan(data[x]) & ~np.isnan(data[y]) & ~np.isnan(data[z])\n",
    "print(inds.sum())\n",
    "\n",
    "rs_xy = stats.spearmanr(data[x][inds], data[y][inds])[0]\n",
    "rs_xz = stats.spearmanr(data[x][inds], data[z][inds])[0]\n",
    "rs_yz = stats.spearmanr(data[y][inds], data[z][inds])[0]\n",
    "print(rs_xy)\n",
    "print(rs_xz)\n",
    "print(rs_yz)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Determine whether the difference in correlation between `Soc-NonSoc_pc` and internalizing vs externalizing symptoms is statistically significant:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n = inds.sum()\n",
    "\n",
    "xy = stats.spearmanr(data[x][inds], data[y][inds])[0]\n",
    "xz = stats.spearmanr(data[x][inds], data[z][inds])[0]\n",
    "yz = stats.spearmanr(data[y][inds], data[z][inds])[0]\n",
    "\n",
    "dependent_corr(xy, xz, yz, n, twotailed=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. RANDOM MECH"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def box_plot(data, edge_color, fill_color,pos,v):\n",
    "    # customize boxplots\n",
    "    alpha= .2\n",
    "    data = data[~np.isnan(data)]\n",
    "    bp = ax.boxplot(data, positions = [pos], patch_artist=True,widths=.6,vert=v,flierprops = dict(markeredgecolor=edge_color,\n",
    "    markerfacecolor=fill_color, alpha=alpha))\n",
    "    \n",
    "    for element in ['boxes', 'whiskers', 'fliers', 'means', 'caps']:\n",
    "        plt.setp(bp[element], color=edge_color)\n",
    "    for element in ['medians']:\n",
    "        plt.setp(bp[element], color='k',linewidth=2,ls='dashed')\n",
    "\n",
    "    for patch in bp['boxes']:\n",
    "        patch.set(facecolor=fill_color,alpha=alpha)#'w'       \n",
    "        \n",
    "    return bp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#red_rgb = [103,0,31] # edges of RdBu\n",
    "#blue_rgb = [5,48,97] # edges of RdBu\n",
    "red_rgb =[188,61,62] # from Emily\n",
    "blue_rgb = [54,122,177] # from Emily\n",
    "red_rgb = np.array(red_rgb)/255\n",
    "blue_rgb = np.array(blue_rgb)/255\n",
    "alpha = .2 # transparency inside boxplots, for datapts etc.\n",
    "\n",
    "#colors = [red_rgb,blue_rgb]\n",
    "# Set your custom color palette\n",
    "#myPalette = sns.set_palette(sns.color_palette(colors))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Look at responses on RANDMECH:\n",
    "all_trial_data = pd.read_csv(os.path.join(behavioral_data_drive, 'hcp_social_826subs.csv'))\n",
    "all_trial_data.set_index(\"subj_idx\", inplace=True)\n",
    "all_trial_data.index.rename(\"Subject\", inplace=True)\n",
    "all_trial_data.index = all_trial_data.index.map(str)\n",
    "all_trial_data.head()\n",
    "\n",
    "data = data.join(all_trial_data[all_trial_data[\"movie\"]==\"Random mechanical.AVI\"][\"response\"])\n",
    "data.rename(columns={\"response\": \"rand_mech_response\"}, inplace=True)\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a new variable denoting subjects who responded either \"yes\" or \"unsure\"\n",
    "data[\"rand_mech_yesorunsure\"] = data[\"rand_mech_response\"] > 0\n",
    "y = \"ASR_Intn_T\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#data1 = data.loc[data['rand_mech_response']!=9,:]\n",
    "#data1.shape\n",
    "fig, ax = plt.subplots(figsize=(5,6))\n",
    "y = \"ASR_Intn_T\"\n",
    "#x = \"rand_mech_response\"\n",
    "x = \"rand_mech_yesorunsure\"\n",
    "rows = data[x] == 0\n",
    "bp2 = box_plot(data.loc[rows,y], blue_rgb, blue_rgb,0,True)\n",
    "print(f'\"Non-social\": Mean int score: {np.nanmean(data.loc[rows,y])},SE: {stats.sem(data.loc[rows,y],nan_policy=\"omit\")}')\n",
    "rows = data[x] == 1\n",
    "bp1 = box_plot(data.loc[rows,y], red_rgb, red_rgb,1,True)\n",
    "print(f'\"Social\" or \"Unsure\": Mean int score: {np.nanmean(data.loc[rows,y])},SE: {stats.sem(data.loc[rows,y],nan_policy=\"omit\")}')\n",
    "plt.xticks(range(2),['\"Non-social\"','\"Social\"/\\n \"Unsure\"'])\n",
    "tscore, p = stats.ttest_ind(data[data[x]==1][y], data[data[x]==0][y], nan_policy='omit')\n",
    "print(f't={tscore}, p={p}')\n",
    "#plt.ylabel('ASR_Intn_T')\n",
    "plt.ylim(30,104)\n",
    "plt.plot([0,0,1,1],[96,98,98,96],linewidth = 1,color='k')\n",
    "plt.plot([0.5],[99]*1,'*',color='k')\n",
    "\n",
    "#ax.annotate(f't = {scoret:.2f}\\n(p = {p:.2g})', xy=(.02,.85), xycoords='axes fraction')\n",
    "plt.title('RANDOM MECH',fontweight='bold')\n",
    "plt.ylabel('Internalizing score (T)')\n",
    "#plt.ylim(0,100)\n",
    "fig.savefig('../results/RANDMECHresp_vs_ASR_Intn_T.png',bbox_inches='tight')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_rand_mech = data[['rand_mech_yesorunsure', 'ASR_Extn_T', 'ASR_Intn_T']]\n",
    "df_rand_mech['int_ext'] = df_rand_mech['ASR_Intn_T'] - df_rand_mech['ASR_Extn_T']\n",
    "\n",
    "x = 'rand_mech_yesorunsure'\n",
    "y = 'int_ext'\n",
    "\n",
    "tscore_rand_intext, p_rand_intext = stats.ttest_ind(df_rand_mech[df_rand_mech[x]==1][y], df_rand_mech[df_rand_mech[x]==0][y], nan_policy='omit')\n",
    "tscore_rand_intext,p_rand_intext"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_rand_mech"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "intn_ = df_rand_mech[['rand_mech_yesorunsure', 'ASR_Intn_T']]\n",
    "intn_['trait_type'] = 'Intn'\n",
    "extn_ = df_rand_mech[['rand_mech_yesorunsure', 'ASR_Extn_T']]\n",
    "extn_['trait_type'] = 'Extn'\n",
    "df_rand_mech_long  = intn_.append(extn_)\n",
    "df_rand_mech_long.reset_index(inplace = True)\n",
    "trait_score = np.empty((df_rand_mech_long.shape[0],))\n",
    "trait_score[:] = np.nan\n",
    "rows = df_rand_mech_long['trait_type']=='Intn'\n",
    "trait_score[rows] = df_rand_mech_long.loc[rows,'ASR_Intn_T']\n",
    "rows = df_rand_mech_long['trait_type']=='Extn'\n",
    "trait_score[rows] = df_rand_mech_long.loc[rows,'ASR_Extn_T']\n",
    "df_rand_mech_long['trait_score'] = trait_score\n",
    "df_rand_mech_long.drop(['ASR_Intn_T', 'ASR_Extn_T'],axis = 1, inplace=True)\n",
    "df_rand_mech_long.rename(columns={'rand_mech_yesorunsure': 'socialness'},  inplace=True)\n",
    "df_rand_mech_long"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "df_rand_mech_long.dropna(inplace=True)\n",
    "df_rand_mech_long.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lmer('trait_score ~  socialness + trait_type + socialness * trait_type + (1|Subject)', data=df_rand_mech_long) # ff: mean response, rf:subjID, movie\n",
    "#model.fit(levels)\n",
    "model.fit(factors={\"socialness\": [\"False\", \"True\"]})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from pymer4.models import Lmer\n",
    "\n",
    "model = Lmer('trait_score ~  socialness + trait_type + socialness * trait_type + (1|Subject)', data=df_rand_mech_long) # ff: mean response, rf:subjID, movie\n",
    "#model.fit(levels)\n",
    "model.fit(factors={\"socialness\": [\"False\", \"True\"]})\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "yes_unsure = df_rand_mech[df_rand_mech[x]==1][y]\n",
    "no = df_rand_mech[df_rand_mech[x]==0][y]\n",
    "yes_unsure.mean(), yes_unsure.sem()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "no.mean(), no.sem()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#data1 = data.loc[data['rand_mech_response']!=9,:]\n",
    "#data1.shape\n",
    "fig, ax = plt.subplots(figsize=(5,6))\n",
    "\n",
    "y = \"ASR_Extn_T\"\n",
    "#x = \"rand_mech_response\"\n",
    "x = \"rand_mech_yesorunsure\"\n",
    "rows = data[x] == 0\n",
    "bp2 = box_plot(data.loc[rows,y], blue_rgb, blue_rgb,0,True)\n",
    "print(f'\"Non-social\": Mean int score: {np.nanmean(data.loc[rows,y])},SE: {stats.sem(data.loc[rows,y],nan_policy=\"omit\")}')\n",
    "rows = data[x] == 1\n",
    "bp1 = box_plot(data.loc[rows,y], red_rgb, red_rgb,1,True)\n",
    "print(f'\"Social\" or \"Unsure\": Mean int score: {np.nanmean(data.loc[rows,y])},SE: {stats.sem(data.loc[rows,y],nan_policy=\"omit\")}')\n",
    "plt.xticks(range(2),['\"Non-social\"','\"Social\"/\\n \"Unsure\"'])\n",
    "tscore, p = stats.ttest_ind(data[data[x]==1][y], data[data[x]==0][y], nan_policy='omit')\n",
    "print(f't={tscore}, p={p}')\n",
    "#plt.ylabel('ASR_Extn_T')\n",
    "plt.plot([0,0,1,1],[93,95,95,93],linewidth = 1,color='k')\n",
    "plt.plot([0.5],[97]*1,'+',color='k')\n",
    "   \n",
    "#ax.annotate(f't = {tscore:.2f}\\n(p = {p:.2g})', xy=(.02,.85), xycoords='axes fraction')\n",
    "plt.title('RANDOM MECH',fontweight='bold')\n",
    "plt.ylabel('Externalizing score (T)')\n",
    "fig.savefig('../results/RANDMECHresp_vs_ASR_Extn_T.png',bbox_inches='tight')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(extra analysis we don't use) Look at RT on RANDMECH:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data = data.join(all_trial_data[all_trial_data[\"movie\"]==\"Random mechanical.AVI\"][\"rt\"])\n",
    "data = data.rename(columns={\"rt\": \"rand_mech_rt\"})\n",
    "data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "g = explore_correlation(x=\"rand_mech_rt\", y=\"ASR_Intn_T\", data=data)\n",
    "# no correlation between RT and internalizing score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Restrict correlation to only subjects that responded \"yes\" or \"unsure\"\n",
    "\n",
    "g = explore_correlation(x=\"rand_mech_rt\", y=\"ASR_Intn_T\", data=data[data[\"rand_mech_yesorunsure\"]==1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# (c) \"Unsure\" responses for Mental and Random vs. ASR_Int "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = \"pc_unc_total\"\n",
    "g = explore_correlation(x, y=\"ASR_Intn_T\")\n",
    "plt.xlabel('Mean(%Unsure\"Social\", %Unsure\"Non-social\")')\n",
    "plt.xlim(0,100)\n",
    "#g.savefig('../results/soc-nonsoc_vs_loneliness.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = \"pc_unc_Rand\"\n",
    "g = explore_correlation(x, y=\"ASR_Intn_T\")\n",
    "plt.ylim(30,100)\n",
    "plt.yticks(10*np.arange(3,11))\n",
    "g.ax_joint.set_ylabel('Internalizing score (T)')#'Mean internalizing beh.\\n (T-score, a.u.)')\n",
    "g.ax_joint.annotate(r\"$ r_{S}=.098$\"f'\\n(p='r\"$.005)$\", xy=(.02,.85), xycoords='axes fraction',fontsize=20) # 0.1, .003\n",
    "g.ax_joint.set_xlabel('%\"Unsure\" Random')\n",
    "g.ax_joint.set_xlim(-3,100)\n",
    "g.savefig('../results/unsure_random_vs_asr_int.png')\n",
    "\n",
    "x = \"pc_unc_Mental\"\n",
    "g = explore_correlation(x, y=\"ASR_Intn_T\")\n",
    "plt.ylim(30,100)\n",
    "plt.yticks(10*np.arange(3,11))\n",
    "g.ax_joint.set_ylabel('Internalizing score (T)')# 'Mean internalizing beh.\\n (T-score, a.u.)')\n",
    "g.ax_joint.annotate(r\"$ r_{S}=.024$\"f'\\n(p='r\"$.49)$\", xy=(.07,.85), xycoords='axes fraction',fontsize=20) # 0.1, .003\n",
    "g.ax_joint.set_xlabel('%\"Unsure\" Mental')\n",
    "g.ax_joint.set_xlim(-3,100)\n",
    "g.savefig('../results/unsure_mental_vs_asr_int.png')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = 'ASR_Intn_T'\n",
    "y = 'pc_unc_Rand'\n",
    "z = 'pc_unc_Mental'\n",
    "\n",
    "inds = ~np.isnan(data[x]) & ~np.isnan(data[y]) & ~np.isnan(data[z])\n",
    "\n",
    "n = inds.sum()\n",
    "xy = stats.spearmanr(data[x][inds], data[y][inds])[0]\n",
    "xz = stats.spearmanr(data[x][inds], data[z][inds])[0]\n",
    "yz = stats.spearmanr(data[y][inds], data[z][inds])[0]\n",
    "\n",
    "dependent_corr(xy, xz, yz, n, twotailed=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y = \"ASR_Extn_T\"\n",
    "'''x = \"pc_unc_total\"\n",
    "g = explore_correlation(x, y)\n",
    "plt.xlabel('Mean(%Unsure\"Social\", %Unsure\"Non-social\")')\n",
    "plt.xlim(0,100)'''\n",
    "#g.savefig('../results/soc-nonsoc_vs_loneliness.png')\n",
    "\n",
    "x = \"pc_unc_Rand\"\n",
    "g = explore_correlation(x, y)\n",
    "plt.xlabel('%Unsure Random')\n",
    "plt.xlim(-1,100)\n",
    "plt.ylim(30,100)\n",
    "plt.yticks(10*np.arange(3,11))\n",
    "plt.ylabel('Mean externalizing beh.\\n (T-score, a.u.)')\n",
    "g.savefig('../results/unsure_random_vs_asr_ext.png')\n",
    "\n",
    "x = \"pc_unc_Mental\"\n",
    "g = explore_correlation(x, y)\n",
    "plt.xlabel('%Unsure Mental')\n",
    "plt.xlim(-1,100)\n",
    "plt.ylim(30,100)\n",
    "plt.yticks(10*np.arange(3,11))\n",
    "plt.ylabel('Mean externalizing beh.\\n (T-score, a.u.)')\n",
    "g.savefig('../results/unsure_mental_vs_asr_ext.png')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compare internalizing and externalizing scores"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = 'pc_unc_Rand'\n",
    "y = 'ASR_Intn_T'\n",
    "z = 'ASR_Extn_T'\n",
    "\n",
    "inds = ~np.isnan(data[x]) & ~np.isnan(data[y]) & ~np.isnan(data[z])\n",
    "\n",
    "n = inds.sum()\n",
    "xy = stats.spearmanr(data[x][inds], data[y][inds])[0]\n",
    "xz = stats.spearmanr(data[x][inds], data[z][inds])[0]\n",
    "yz = stats.spearmanr(data[y][inds], data[z][inds])[0]\n",
    "\n",
    "dependent_corr(xy, xz, yz, n, twotailed=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = 'pc_unc_Mental'\n",
    "y = 'ASR_Intn_T'\n",
    "z = 'ASR_Extn_T'\n",
    "\n",
    "inds = ~np.isnan(data[x]) & ~np.isnan(data[y]) & ~np.isnan(data[z])\n",
    "\n",
    "n = inds.sum()\n",
    "xy = stats.spearmanr(data[x][inds], data[y][inds])[0]\n",
    "xz = stats.spearmanr(data[x][inds], data[z][inds])[0]\n",
    "yz = stats.spearmanr(data[y][inds], data[z][inds])[0]\n",
    "\n",
    "dependent_corr(xy, xz, yz, n, twotailed=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trying LME"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pymer4.models import Lmer, Lm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#data.head()\n",
    "list(data.columns)[:5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.tile([1,2],10), np.repeat([1,2],10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nan_rows = (np.isnan(data['pc_unc_Rand'])) | (np.isnan(data['pc_unc_Mental']))\n",
    "data1 = data.loc[~nan_rows,:]\n",
    "data.shape, data1.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "subs = list(data.index)\n",
    "pc_Uns_Rand = data['pc_unc_Rand']\n",
    "pc_Uns_Mental = data['pc_unc_Mental']\n",
    "ASR_Intn_T = data['ASR_Intn_T']\n",
    "ASR_Extn_T = data['ASR_Extn_T']\n",
    "\n",
    "subs_all = np.tile(subs,2)\n",
    "cond = []\n",
    "cond.extend(np.repeat('Random',len(subs)))\n",
    "cond.extend(np.repeat('Mental',len(subs)))\n",
    "pc_Unsure = []\n",
    "pc_Unsure.extend(pc_Uns_Rand)\n",
    "pc_Unsure.extend(pc_Uns_Mental)\n",
    "\n",
    "df = pd.DataFrame({'Subs':subs_all, 'cond': cond, 'pc_Unsure': pc_Unsure, 'Int': np.tile(ASR_Intn_T,2), 'Ext': np.tile(ASR_Extn_T,2)})\n",
    "#df['Int_Ext'] = df ['Int'] - df['Ext']\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1 = pd.melt(df, id_vars = ['Subs','cond', 'pc_Unsure'], var_name = 'trait_type', value_name = 'trait_score')#, value_vars=['Int','Ext'])\n",
    "df1.dropna(inplace=True)\n",
    "df1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lmer(' pc_Unsure  ~  trait_type + trait_score + cond  + trait_type*trait_score*cond + (1|Subs)', data=df1) # ff: mean response, rf:subjID, movie\n",
    "#model.fit()\n",
    "model.fit(factors={\"cond\": [\"Random\", \"Mental\"]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lm(' pc_Unsure  ~  trait_type + trait_score + stimType + trait_type*trait_score + trait_type*stimType + trait_score*stimType + trait_type*trait_score*stimType ', data=df1) # ff: mean response, rf:subjID, movie\n",
    "#model.fit()\n",
    "model.fit()#factors={\"cond\": [\"Random\", \"Mental\"]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df1.rename(columns={'cond':'stimType'}, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lmer(' pc_Unsure  ~  trait_type + trait_score + stimType + trait_type*trait_score + trait_type*stimType + trait_score*stimType + trait_type*trait_score*stimType + (1|Subs)', data=df1) # ff: mean response, rf:subjID, movie\n",
    "#model.fit()\n",
    "model.fit(factors={\"stimType\": [\"Random\", \"Mental\"]})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''model = Lmer('trait_score ~  trait_type + pc_Unsure + cond + trait_type*pc_Unsure + trait_type*cond + pc_Unsure*cond + trait_type*cond*pc_Unsure + (1|Subs)', data=df1) # ff: mean response, rf:subjID, movie\n",
    "#model.fit()\n",
    "model.fit(factors={\"cond\": [\"Random\", \"Mental\"]})'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_MentRand = data[['pc_unc_Rand', 'pc_unc_Mental','ASR_Intn_T', 'ASR_Extn_T']]\n",
    "df_MentRand['Rand_Ment'] = df_MentRand['pc_unc_Rand'] - df_MentRand['pc_unc_Mental'] \n",
    "df_MentRand"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "intn_ = df_MentRand[['Rand_Ment','ASR_Intn_T']]\n",
    "intn_['trait_type'] = 'Int'\n",
    "extn_ = df_MentRand[['Rand_Ment','ASR_Extn_T']]\n",
    "extn_['trait_type'] = 'Ext'\n",
    "\n",
    "df_long1  = intn_.append(extn_)\n",
    "df_long1.reset_index(inplace = True)\n",
    "trait_score = np.empty((df_long1.shape[0],))\n",
    "trait_score[:] = np.nan\n",
    "rows = df_long1['trait_type']=='Int'\n",
    "trait_score[rows] = df_long1.loc[rows,'ASR_Intn_T']\n",
    "rows = df_long1['trait_type']=='Ext'\n",
    "trait_score[rows] = df_long1.loc[rows,'ASR_Extn_T']\n",
    "df_long1['trait_score'] = trait_score\n",
    "df_long1.drop(['ASR_Intn_T', 'ASR_Extn_T'],axis = 1, inplace=True)\n",
    "df_long1.dropna(inplace = True)\n",
    "df_long1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.scatterplot(x = 'trait_score', y = 'Rand_Ment', hue = 'trait_type', data = df_long1)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#model = Lmer('Rand_Ment ~  trait_type + trait_score + trait_type*trait_score + (1|Subject)', data=df_long1) # ff: mean response, rf:subjID, movie\n",
    "model = Lmer('trait_score ~  trait_type + Rand_Ment + trait_type*Rand_Ment + (1|Subject)', data=df_long1) # ff: mean response, rf:subjID, movie\n",
    "model.fit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lmer('Rand_Ment ~  trait_type + trait_score + trait_type*trait_score + (1|Subject)', data=df_long1) # ff: mean response, rf:subjID, movie\n",
    "#model = Lmer('trait_score ~  trait_type + Rand_Ment + trait_type*Rand_Ment + (1|Subject)', data=df_long1) # ff: mean response, rf:subjID, movie\n",
    "model.fit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_MentRand = pd.DataFrame({'Subs':subs_all, 'cond': cond, 'pc_Unsure': pc_Unsure, 'Int': np.tile(ASR_Intn_T,2), 'Ext': np.tile(ASR_Extn_T,2)})\n",
    "df['Int_Ext'] = df ['Int'] - df['Ext']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nan_rows = np.isnan(df['pc_Unsure']) | np.isnan(df['Int']) | np.isnan(df['Ext'])\n",
    "df = df.loc[~nan_rows,:]\n",
    "nan_rows1 = np.isnan(df['pc_Unsure']) | np.isnan(df['Int']) | np.isnan(df['Ext'])\n",
    "np.where(nan_rows), np.where(nan_rows1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lmer('pc_Unsure ~  cond + (1|Subs)', data=df) # ff: mean response, rf:subjID, movie\n",
    "model.fit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "intn_ = df[['pc_Unsure', 'cond', 'Int', 'Subs']]\n",
    "intn_['trait_type'] = 'Int'\n",
    "extn_ = df[['pc_Unsure', 'cond', 'Ext', 'Subs']]\n",
    "extn_['trait_type'] = 'Ext'\n",
    "df_long  = intn_.append(extn_)\n",
    "df_long.reset_index(inplace = True)\n",
    "trait_score = np.empty((df_long.shape[0],))\n",
    "trait_score[:] = np.nan\n",
    "rows = df_long['trait_type']=='Int'\n",
    "trait_score[rows] = df_long.loc[rows,'Int']\n",
    "rows = df_long['trait_type']=='Ext'\n",
    "trait_score[rows] = df_long.loc[rows,'Ext']\n",
    "df_long['trait_score'] = trait_score\n",
    "df_long.drop(['Int', 'Ext'],axis = 1, inplace=True)\n",
    "\n",
    "#df_long.dropna(inplace=True)\n",
    "#df_long.shape\n",
    "df_long\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_long.dropna(inplace=True)\n",
    "df_long.shaape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_long.loc[df_long['trait_type']=='Int',:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "odel = Lmer('pc_Unsure ~  cond + trait_score + cond * trait_score + (1|Subs)', data=df_long.loc[df_long['trait_type']=='Int',:]) # ff: mean response, rf:subjID, movie\n",
    "#model.fit(levels)\n",
    "model.fit(factors={\"cond\": [\"Random\", \"Mental\"]})\n",
    "\n",
    "# TO BE CONTINUED!!!\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "odel = Lmer('pc_Unsure ~  cond + trait_score + cond * trait_score + (1|Subs)', data=df_long.loc[df_long['trait_type']=='Int',:]) # ff: mean response, rf:subjID, movie\n",
    "#model.fit(levels)\n",
    "model.fit(factors={\"cond\": [\"Random\", \"Mental\"]})\n",
    "\n",
    "# TO BE CONTINUED!!!\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lmer('pc_Unsure ~  cond + trait_score + cond * trait_score + (1|Subs)', data=df_long.loc[df_long['trait_type']=='Ext',:]) # ff: mean response, rf:subjID, movie\n",
    "#model.fit(levels)\n",
    "model.fit(factors={\"cond\": [\"Random\", \"Mental\"]})\n",
    "\n",
    "# TO BE CONTINUED!!!\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lmer('pc_Unsure ~  cond + trait_score + trait_type + cond * trait_score + (1|Subs)', data=df_long) # ff: mean response, rf:subjID, movie\n",
    "#model.fit(levels)\n",
    "model.fit(factors={\"cond\": [\"Random\", \"Mental\"]})\n",
    "\n",
    "# TO BE CONTINUED!!!\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lmer('trait_score ~  pc_Unsure + cond + cond * pc_Unsure + (1|Subject)', data=pc_Unsure) # ff: mean response, rf:subjID, movie\n",
    "#model.fit(levels)\n",
    "model.fit(factors={\"socialness\": [\"False\", \"True\"]})\n",
    "\n",
    "# TO BE CONTINUED!!!\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lmer('pc_Unsure ~  Int_Ext + (1|Subs)', data=df) # ff: mean response, rf:subjID, movie\n",
    "model.fit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lm('pc_Unsure ~  cond + Int_Ext + cond*Int_Ext', data=df) # ff: mean response, rf:subjID, movie\n",
    "model.fit(factors={\"cond\":[\"Random\",\"Mental\"]})\n",
    "#model.fit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lm('pc_Unsure ~  cond + Int*cond', data=df) # ff: mean response, rf:subjID, movie\n",
    "model.fit(factors={\"cond\":[\"Random\",\"Mental\"]})\n",
    "#model.fit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lm('pc_Unsure ~  cond + Ext*cond', data=df) # ff: mean response, rf:subjID, movie\n",
    "model.fit(factors={\"cond\":[\"Random\",\"Mental\"]})\n",
    "#model.fit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lm('pc_Unsure ~  cond + Int + Ext', data=df) # ff: mean response, rf:subjID, movie\n",
    "model.fit(factors={\"cond\":[\"Random\",\"Mental\"]})\n",
    "#model.fit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Lmer('pc_Unsure ~  cond + Int_Ext + cond*Int_Ext + (1|Subs)', data=df) # ff: mean response, rf:subjID, movie\n",
    "model.fit(factors={\"cond\":[\"Random\",\"Mental\"]})\n",
    "#model.fit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30fd9c97283ec1278eec212a8f8afab06ad903f38228c32cacb469eba8e56f4f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('py37': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}